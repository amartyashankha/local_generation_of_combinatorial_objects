\section{Local-Access Implementations for Random Undirected Graphs}
\label{sec:undirected}

In this section, we provide an efficient local access implementations for random undirected graphs
when the probabilities $p_{uv}=\mathbb P[(u,v)\in E]$ are given, and we can efficiently approximate the following quantities:
(1) the probability that there is no edge between a vertex $u$ and a range of consecutive vertices $[a,b]$, namely $\prod_{u=a}^b (1-p_{vu})$, and
(2) the sum of the edge probabilities (i.e., the expected number of edges) between $u$ and vertices from $[a,b]$, namely $\sum_{u=a}^b p_{vu}$.
In Section~\ref{sec:applications}, we provide subroutines for computing these values for the Erd\"{o}s-R\'{e}nyi model and the Stochastic Block model.
We also begin by assuming perfect-precision arithmetic, which we relax in Section~\ref{sec:remove-perfect}.

\iffalse
{\color{blue}
Alternatively, we provide an implementation for these two models with a deterministic performance guarantee in Section~\ref{sec:ER-det}.
In this setting, introducing the \func{Vertex-Pair} queries results in an amortized guarantee on the run-time.
The deterministic guarantee comes at the cost of more complicated data-structures
(we use a two-level nested interval tree and binary search tree).
%We then give two modifications to this implementation; a simplified generator that provides a randomized guarantee in Section~\ref{sec:ER-rand}, and an augmented generator that also implements \func{Vertex-Pair} queries in Section~\ref{sec:ER-pair}.
}
\fi

First, consider the adjacency matrix $\ADJ$ of $G$ where each entry $\ADJ[u][v]$ can exist in three possible states:
$\ADJ[u][v] = \ONE$ or $\ZERO$ if the algorithm has determined that $\{u,v\}\in E$ or $\{u,v\} \notin E$ respectively,
and $\ADJ[u][v] = \PHI$ if whether ${u,v}\in E$ or not will be determined by future random choices
(in fact, the marginal probability of $\mathbb P[(u,v)\in E]$ conditioned on all prior samples is still $p_{uv}$).
Our implementation also maintains the vector $\LAST$ (used in the same sense as \cite{reut}),
where $\LAST[v]$ records the neighbor of $v$ returned in the last call \func{Next-Neighbor}$(v)$, or $\LAST[v]=0$ if no such call has been invoked.
All cells of $\ADJ$ and $\LAST$ are initialized to $\PHI$ and $0$, respectively.

We use the Bernoulli random variable $X_{uv} \sim \mathsf{Bern}(p_{uv})$ when sampling the value of $\ADJ[u][v]=\phi$.
For the sake of analysis, we will frequently view our random process as if the \emph{entire} table of random variables $X_{uv}$ has been sampled
\emph{up-front} (i.e. all coins have already been flipped), and the algorithm simply ``uncovers'' these variables instead of making coin-flips.
Thus, every cell $\ADJ[u][v]$ is originally $\PHI$, but will eventually take the value $X_{uv}$ once the graph generation is complete.

\paragraph*{Obstacles for maintaining $\ADJ$ explicitly}
First, consider a naive implemention that fills out the cells of $\ADJ$ one-by-one as required by each query;
equivalently, we perform $\func{Vertex-Pair}$ queries on successive vertices until a neighbor is found.
There are two problems with this approach.
Firstly, the algorithm only finds a neighbor, for a \func{Random-Neighbor} or \func{Next-Neighbor} query, with probability $p_{uv}$,
which requires too many iterations: for $G(n,p)$ this requires $1/p$ iterations, which is already infeasible for $p = o(1/\poly(\log n))$.
Secondly, the algorithm may generate a large number of non-neighbors in the process, possibly in random or arbitrary locations.




\subsection{\func{Next-Neighbor} Queries via Run-of-$\ZERO$'s Sampling}\label{sec:ER-rand}

We implement \func{Next-Neighbor}$(v)$ by sampling for the first index $u > \LAST[v]$ such that $X_{vu}=\ONE$,
from a sequence of Bernoulli RVs $\{X_{v, u}\}_{u > \LAST[v]}$.
To do so, we sample a consecutive ``run'' of $\ZERO$'s with probability $\prod_{u=\LAST[v]+1}^{u'} (1-p_{vu})$:
this is the probability that there is no edge between a vertex $v$ and any $u \in (\LAST[v],u']$, which can be computed efficiently by our assumption.
The problem is that, some entries $\ADJ[v][u]$'s in this run may have already been determined (to be $\ONE$ or $\ZERO$)
by queries $\func{Next-Neighbor}(u)$ for $u > \LAST[v]$.
To mitigate this issue, we give a succinct data structure that determines the value of $\ADJ[v][u]$ for $u >\LAST[v]$ and,
more generally, captures the state $\ADJ$, in Section~\ref{sec:nn-ds}.
Using this data structure, we ensure that our sampled run does not skip over any $\ONE$.
Next, for the sampled index $u$ of the first occurrence of $\ONE$,
we check against this data structure to see if $\ADJ[v][u]$ is already assigned to $\ZERO$, in which case we re-sample for a new candidate $u' > u$.
Section~\ref{sec:nn-correctness} discusses the subtlety of this issue.

We note that we do not yet try to handle other types of queries here yet.
We also do not formally bound the number of re-sampling iterations of this approach here, because the argument is not needed by our final algorithm.
Yet, we remark that $O(\log n)$ iterations suffice with high probability, even if the queries are adversarial.
This method can be extended to support \func{Vertex-Pair} queries (but unfortunately not \func{Random-Neighbor} queries).
See Section~\ref{sec:reroll-cont} for full details.

\subsubsection{Data structure}\label{sec:nn-ds}

From the definition of $X_{uv}$, \func{Next-Neighbor}$(v)$ is given by $\min\{u > \LAST[v]: X_{vu} = \ONE\}$.
Let $P_v = \{u: \ADJ[v][u]=1\}$ be the set of known neighbors of $v$, and $w_v = \min \{(P_v \cap (\LAST[v], n])\}$ be its first known neighbor
not yet reported by a $\func{Next-Neighbor}(v)$ query, or equivalently, the next occurrence of $\ONE$ in $v$'s row on $\ADJ$ after $\LAST[v]$.
If there is no known neighbor of $v$ after $\LAST[v]$, we set $w_v = n+1$.
Consequently, $\ADJ[v][u] \in \{\PHI,\ZERO\}$ for all $u \in (\LAST[v], w_v)$,
so \func{Next-Neighbor}$(v)$ is either the index $u$ of the first occurrence of $X_{vu} = \ONE$ in this range, or $w_v$ if no such index exists.

We keep track of $\LAST[v]$ in a dictionary, to avoid any initialization overhead.
Each $P_v$ is maintained as an ordered set, which is also instantiated when it becomes non-empty.
When \func{Next-Neighbor}$(v)$ returns $u$, we add $v$ to $P_u$ and $u$ to $P_v$.
We do not attempt to maintain $\ADJ$ explicitly, as updating it requires replacing up to $\Theta(n)$ $\PHI$'s to $\ZERO$'s
for a single \func{Next-Neighbor} query in the worst case.
Instead, we argue that $\LAST$ and $P_v$'s provide a succinct representation of $\ADJ$ via the following observation.

\begin{restatable}{lemma}{cond}\label{lem:cond-0}
The data structures $\LAST$ and $P_v$'s together provide a succinct representation of $\ADJ$ when only \func{Next-Neighbor} queries are allowed. In particular, $\ADJ[v][u]=\ONE$ if and only if $u \in P_v$. Otherwise, $\ADJ[v][u]=\ZERO$ when $u <\LAST[v]$ or $v < \LAST[u]$. In all remaining cases, $\ADJ[v][u]=\PHI$.
\end{restatable}
\begin{proof}
The condition for $\ADJ[v][u]=\ONE$ clearly holds by constuction.
Otherwise, observe that $\ADJ[v][u]$ becomes \emph{decided} (i.e. its value is changed from $\PHI$ to $\ZERO$)
during the first call to \func{Next-Neighbor}$(v)$ that returns a value $u' > u$ thereby setting $\LAST[v] = u' \implies u<\LAST[v]$, or vice versa.
\end{proof}



\subsubsection{Queries and Updates}
\label{sec:nn-correctness}
\begin{wrapfigure}[14]{R}{0.42\textwidth}
\vspace{-1em}
\begin{framed}
    \renewcommand\figurename{Algorithm}
    \caption{Sampling \func{Next-Neighbor}}
    \label{alg:oblivious-coin-toss}
    \begin{algorithmic}[1]
        \Procedure{Next-Neighbor}{$v$}
            \State{$u \gets \LAST[v]$}
            \State{$w_v \gets \min \{(P_v \cap (u, n]) \cup \{n+1\}\}$}
            \While{$u = w_v$ or $\LAST[u] < v$}
                \State{\textbf{sample} $F\sim\mathsf{F}(v,u,w_v)$}
                \State{$u \gets F$}
            \EndWhile
            \If{$u \neq w_v$}
                \State{$P_v \gets P_v \cap \{u\}$}
                \State{$P_u \gets P_u \cap \{v\}$}
            \EndIf
            \State{$\LAST[v] \gets u$}
            \State \Return $u$
        \EndProcedure
    \end{algorithmic}
\end{framed}
\end{wrapfigure}
We now present Algorithm~\ref{alg:oblivious-coin-toss}, and discuss the correctness of its sampling process.
The argument here is rather subtle and relies on viewing the random process as an ``uncovering'' process on the table of RVs $X_{uv}$
(introduced in Section~\ref{sec:undirected}).
Consider the following experiment for sampling the next neighbor of $v$ in the range $(\LAST[v],w_v)$.
Suppose that we generate a sequence of $w_v-\LAST[v]-1$ independent coin-tosses,
where the $i^{th}$ coin $C_{vu}$ corresponding to $u = \LAST[v]+i$ has bias $p_{vu}$, regardless of whether $X_{vu}$'s are decided or not.
Then, we use the sequence $\langle C_{vu} \rangle$ to assign values to \emph{undecided} random variable $X_{vu}$.
The main observation here is that, the \emph{decided} random variables $X_{vu} = \ZERO$ do not need coin-flips,
and the corresponding coin result $C_{vu}$ can simply be discarded.
Thus, we generate coin-flips until we encounter some $u$ satisfying both $C_{vu} = \ONE$ and $\ADJ[v][u] = \PHI$.

Let $\mathsf{F}(v,a,b)$ denote the probability distribution of the occurrence $u$ of the first coin-flip $C_{vu} = \ONE$ among the neighbors in $(a, b)$.
More specifically, $F\sim\mathsf{F}(v,a,b)$ represents the event that $C_{v,a+1}=\cdots=C_{v,F-1}=\ZERO$ and $C_{v,F}=\ONE$,
which happens with probability $\mathbb P[F=f]=\prod_{u=a+1}^{f-1} (1-p_{vu}) \cdot p_{vf}$.
For convenience, let $F = b$ denote the event where all $C_{vu}=\ZERO$. Our algorithm samples $F_1\sim\mathsf{F}(v,\LAST[v],w_v)$ to find the first occurrence of $C_{v,F_1} = \ONE$, then samples $F_2\sim\mathsf{F}(v,F_1,w_v)$ to find the second occurrence $C_{v,F_2} = \ONE$, and so on.
These values $\{F_i\}$ are iterated as $u$ in Algorithm~\ref{alg:oblivious-coin-toss}.
As this process generates $u$ satisfying $C_{vu}=1$ in the increasing order, we repeat until we find one that also satisfies $\ADJ[u][v]=\phi$.
Note that once the process terminates at some $u$, we make no implications on the results of any uninspected coin-flips after $C_{vu}$.

\paragraph*{Obstacles for extending beyond \func{Next-Neighbor} queries}
There are two main issues that prevent this method from supporting \func{Random-Neighbor} queries.
Firstly, while one might consider applying \func{Next-Neighbor} starting from some random location $u$ to find the minimum $u' \geq u$
where $\ADJ[v][u']=\ONE$, the probability of choosing $u'$ will depend on the probabilities $p_{vu}$'s, and is generally not uniform.
Secondly, in Section~\ref{sec:nn-ds}, we observe that $\LAST[v]$ and $P_v$ together provide a succinct representation of $\ADJ[v][u] = \ZERO$
only for contiguous cells $\ADJ[v][u]$ where $u \leq \LAST[v]$ or $v \leq \LAST[u]$: they cannot handle $\ZERO$ anywhere else.
Unfortunately, in order to support \func{Random-Neighbor} queries, we will likely need to assign $\ADJ[v][u]$ to $\ZERO$ in random locations
beyond $\LAST[v]$ or $\LAST[u]$, which cannot be captured by the current data structure.
Specifically, to speed-up the sampling process for small $p_{vu}$'s, we must generate many random non-neighbors at once,
but we cannot afford to spend time linear in the number of created $\ZERO$'s to update our data structure.
We remedy these issues via the following bucketing approach.
