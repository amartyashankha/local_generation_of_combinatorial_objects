\section{Local-Access Implementations for Random Undirected Graphs}
\label{sec:undirected}

In this section, we provide an efficient local access implementations for random undirected graphs
when the probabilities $p_{uv}=\mathbb P[(u,v)\in E]$ are given, and we can efficiently approximate the following quantities:
(1) the probability that there is no edge between a vertex $u$ and a range of consecutive vertices $[a,b]$, namely $\prod_{u=a}^b (1-p_{vu})$, and
(2) the sum of the edge probabilities (i.e., the expected number of edges) between $u$ and vertices from $[a,b]$, namely $\sum_{u=a}^b p_{vu}$.
In Section~\ref{sec:applications}, we provide subroutines for computing these values for the Erd\"{o}s-R\'{e}nyi model and the Stochastic Block model.
We also begin by assuming perfect-precision arithmetic, which we relax in Section~\ref{sec:remove-perfect}.

\iffalse
{\color{blue}
Alternatively, we provide an implementation for these two models with a deterministic performance guarantee in Section~\ref{sec:ER-det}.
In this setting, introducing the \func{Vertex-Pair} queries results in an amortized guarantee on the run-time.
The deterministic guarantee comes at the cost of more complicated data-structures
(we use a two-level nested interval tree and binary search tree).
%We then give two modifications to this implementation; a simplified generator that provides a randomized guarantee in Section~\ref{sec:ER-rand}, and an augmented generator that also implements \func{Vertex-Pair} queries in Section~\ref{sec:ER-pair}.
}
\fi

First, consider the adjacency matrix $\ADJ$ of $G$ where each entry $\ADJ[u][v]$ can exist in three possible states:
$\ADJ[u][v] = \ONE$ or $\ZERO$ if the generator has determined that $\{u,v\}\in E$ or $\{u,v\} \notin E$ respectively,
and $\ADJ[u][v] = \PHI$ if the marginal probability of $\mathbb P[(u,v)\in E]$ conditioned on all prior samples is $p_{uv}$.
Our generator also maintains the vector $\LAST$ (used in the same sense as \cite{reut}),
where $\LAST[v]$ records the neighbor of $v$ returned in the last call \func{Next-Neighbor}$(v)$, or $\LAST[v]=0$ if no such call has been invoked.
All cells of $\ADJ$ and $\LAST$ are initialized to $\PHI$ and $0$, respectively.

We use the Bernoulli random variable $X_{uv} \sim \mathsf{Bern}(p_{uv})$ when sampling the value of $\ADJ[u][v]=\phi$.
For the sake of analysis, we will frequently consider the \emph{entire} table of RVs $X_{uv}$ being sampled \emph{up-front} (i.e., flip all coins),
and the algorithm simply ``uncovers'' these variables instead of making coin-flips.
Thus, every cell $\ADJ[u][v]$ is originally $\PHI$, but will eventually take the value $X_{uv}$ once the graph generation is complete.

\paragraph*{Obstacles for maintaining $\ADJ$}
First, consider a naive implemention that fills out the cells of $\ADJ$ one-by-one as required by each query.
There are two problems with this approach.
Firstly, the algorithm only finds a neighbor, for a \func{Random-Neighbor} or \func{Next-Neighbor} query, with probability $p_{uv}$,
which requires too many iterations: for $G(n,p)$ this requires $1/p$ iterations, which is already infeasible for $p = o(1/\poly(\log n))$.
Secondly, the algorithm may generate a large number of non-neighbors in the process, possibly in random or arbitrary locations.




\subsection{\func{Next-Neighbor} Queries via Run-of-$\ZERO$'s Sampling}\label{sec:ER-rand}

We implement \func{Next-Neighbor}$(v)$ by sampling for the first index $u > \LAST[v]$ such that $X_{vu}=\ONE$,
from a sequence of Bernoulli RVs $\{X_{v, u}\}_{u > \LAST[v]}$.
To do so, we sample a consecutive ``run'' of $\ZERO$'s with probability $\prod_{u=\LAST[v]+1}^{u'} (1-p_{vu})$:
this is the probability that there is no edge between a vertex $v$ and any $u \in (\LAST[v],u']$, which can be computed efficiently by our assumption.
The problem is that, some entries $\ADJ[v][u]$'s in this run may have already been determined (to be $\ONE$ or $\ZERO$)
by queries $\func{Next-Neighbor}(u)$ for $u > \LAST[v]$.
To mitigate this issue, we give a succinct data structure that determines the value of $\ADJ[v][u]$ for $u >\LAST[v]$ and,
more generally, captures the state $\ADJ$, in Section~\ref{sec:nn-ds}.
Using this data structure, we ensure that our sampled run does not skip over any $\ONE$.
Next, for the sampled index $u$ of the first occurrence of $\ONE$,
we check against this data structure to see if $\ADJ[v][u]$ is already assigned to $\ZERO$, in which case we re-sample for a new candidate $u' > u$.
Section~\ref{sec:nn-correctness} discusses the subtlety of this issue.

We note that we do not yet try to handle other types of queries here yet.
We also do not formally bound the number of re-sampling iterations of this approach here, because the argument is not needed by our final algorithm.
Yet, we remark that $O(\log n)$ iterations suffice with high probability, even if the queries are adversarial.
This method can be extended to support \func{Vertex-Pair} queries (but unfortunately not \func{Random-Neighbor} queries).
See Section~\ref{sec:reroll-cont} for full details.

\subsubsection{Data structure}\label{sec:nn-ds}

From the definition of $X_{uv}$, \func{Next-Neighbor}$(v)$ is given by $\min\{u > \LAST[v]: X_{vu} = \ONE\}$ (or $n+1$ if no satisfying $u$ exists). Let
$P_v = \{u: \ADJ[v][u]=1\}$ be the set of known neighbors of $v$, and $w_v = \min \{(P_v \cap (\LAST[v], n]) \cup \{n+1\}\}$ be its first known neighbor not yet reported by a $\func{Next-Neighbor}(v)$ query, or equivalently, the next occurrence of $\ONE$ in $v$'s row on $\ADJ$ after $\LAST[v]$. Note that $w_v = n+1$ denotes that there is no known neighbor of $v$ after $\LAST[v]$.
Consequently, $\ADJ[v][u] \in \{\PHI,\ZERO\}$ for all $u \in (\LAST[v], w_v)$, so \func{Next-Neighbor}$(v)$ is
either the index $u$ of the first occurrence of $X_{vu} = \ONE$ in this range, or $w_v$ if no such index exists.

We keep track of $\LAST[v]$ in a dictionary, where the key-value pair $(v,\LAST[v])$ is stored only when $\LAST[v] \neq 0$: this removes any initialization overhead.
Each $P_v$ is maintained as an ordered set, which is also only instantiated when it becomes non-empty.
We maintain $P_v$ simply by adding $u$ to $v$ if a call \func{Next-Neighbor}$(v)$ returns $u$, and vice versa. Clearly, $\ADJ[v][u]=1$ if and only if $u \in P_v$ by construction.

As discussed in the previous section, we cannot maintain $\ADJ$ explicitly, as updating it requires replacing up to $\Theta(n)$ $\PHI$'s to $\ZERO$'s for a single \func{Next-Neighbor} query in the worst case. Instead, we argue that $\LAST$ and $P_v$'s provide a succinct representation of $\ADJ$ via the following observation. For simplicity, we say that $X_{uv}$ is \emph{decided} if $\ADJ[u][v] \neq \PHI$, and call it \emph{undecided} otherwise.

\begin{restatable}{lemma}{cond}\label{lem:cond-0}
The data structures $\LAST$ and $P_v$'s together provide a succinct representation of $\ADJ$ when only \func{Next-Neighbor} queries are allowed. In particular, $\ADJ[v][u]=\ONE$ if and only if $u \in P_v$. Otherwise, $\ADJ[v][u]=\ZERO$ when $u <\LAST[v]$ or $v < \LAST[u]$. In all remaining cases, $\ADJ[v][u]=\PHI$.
\end{restatable}
\begin{proof}
The condition for $\ADJ[v][u]=\ONE$ clearly holds by constuction. Otherwise, observe that $\ADJ[v][u]$ becomes decided (that is, its value is changed from $\PHI$ to $\ZERO$) precisely during the first call of \func{Next-Neighbor}$(v)$ that returns a value $u' > u$ which thereby sets $\LAST[v]$ to $u'$ yielding $u<\LAST[v]$, or vice versa.
\end{proof}

\begin{wrapfigure}[15]{R}{0.5\textwidth}
\vspace{-2em}
\begin{framed}
    \renewcommand\figurename{Algorithm}
    \caption{Sampling \func{Next-Neighbor}}
    \label{alg:oblivious-coin-toss}
    \begin{algorithmic}[1]
        \Procedure{Next-Neighbor}{$v$}
            \State{$u \gets \LAST[v]$}
            \State{$w_v \gets \min \{(P_v \cap (u, n]) \cup \{n+1\}\}$}
            \While{$u = w_v$ or $\LAST[u] < v$}
                \State{\textbf{sample} $F\sim\mathsf{F}(v,u,w_v)$}
                \State{$u \gets F$}
            \EndWhile
            \If{$u \neq w_v$}
                \State{$P_v \gets P_v \cap \{u\}$}
                \State{$P_u \gets P_u \cap \{v\}$}
            \EndIf
            \State{$\LAST[v] \gets u$}
            \State \Return $u$
        \EndProcedure
    \end{algorithmic}
\end{framed}
\end{wrapfigure}



\subsubsection{Queries and Updates}\label{sec:nn-correctness}
We now provide our generator (Algorithm~\ref{alg:oblivious-coin-toss}), and discuss the correctness of its sampling process. The argument here is rather subtle and relies on viewing the random process as an ``uncovering'' process on the table of RVs $X_{uv}$'s as previously introduced in Section~\ref{sec:ER-naive}.
Algorithm~\ref{alg:oblivious-coin-toss}, considers the following experiment for sampling the next neighbor of $v$ in the range $(\LAST[v],w_v)$.
Suppose that we generate a sequence of $w_v-\LAST[v]-1$ independent coin-tosses,
where the $i^\textrm{th}$ coin $C_{vu}$ corresponding to $u = \LAST[v]+i$ has bias $p_{vu}$,
regardless of whether $X_{vu}$'s are decided or not.
Then, we use the sequence $\langle C_{vu} \rangle$ to assign values to \emph{undecided} random variable $X_{vu}$.
The crucial observation here is that, the \emph{decided} random variables $X_{vu} = \ZERO$ do not need coin-flips, and the corresponding coin result $C_{vu}$ can simply be discarded.
Thus, we need to generate coin-flips up until we encounter some $u$ satisfying
both (i) $C_{vu} = \ONE$, and (ii) $\ADJ[v][u] = \PHI$.

%\anak{Due to Shankha's change, $\mathsf{F}$ here and $\mathsf{ExactF}$ in the deterministic construction have rather different meaning; consider using a different name altogether in the latter.}

Let $\mathsf{F}(v,a,b)$ denote the probability distribution of the occurrence $u$ of the first coin-flip $C_{vu} = \ONE$ among the neighbors in $(a, b)$.
More specifically, $F\sim\mathsf{F}(v,a,b)$ represents the event that $C_{v,a+1}=\cdots=C_{v,F-1}=\ZERO$ and $C_{v,F}=\ONE$,
which happens with probability $\mathbb P[F=f]=\prod_{u=a+1}^{f-1} (1-p_{vu}) \cdot p_{vf}$.
For convenience, let $F = b$ denote the event where all $C_{vu}=\ZERO$. Our algorithm samples $F_1\sim\mathsf{F}(v,\LAST[v],w_v)$ to find the first occurrence of $C_{v,F_1} = \ONE$, then samples $F_2\sim\mathsf{F}(v,F_1,w_v)$ to find the second occurrence $C_{v,F_2} = \ONE$, and so on.
These values $\{F_i\}$ are iterated as $u$ in Algorithm~\ref{alg:oblivious-coin-toss}.
As this process generates $u$ satisfying (i) in the increasing order, we repeat until we find one that also satisfies (ii).
Note that once the process terminates at some $u$, we make no implications on the results of any uninspected coin-flips after $C_{vu}$.

\paragraph*{Obstacles for extending beyond \func{Next-Neighbor} queries}
There are two main issues that prevent this method from supporting \func{Random-Neighbor} queries.
Firstly, while one might consider applying \func{Next-Neighbor} from some random location $u$ to find the minimum $u' \geq u$ where $\ADJ[v][u']=\ONE$, the probability of choosing $u'$ will depend on the probabilities $p_{vu}$'s, and is generally not uniform.
While a rejection sampling method may be applied to balance out the probabilities of choosing neighbors, these arbitrary $p_{vu}$'s may distribute the neighbors rather unevenly: some small contiguous locations may contain so many neighbors that the rejection sampling approach requires too many iterations to obtain a single uniform neighbor.

 Secondly, in developing Algorithm~\ref{alg:oblivious-coin-toss}, we observe that $\LAST[v]$ and $P_v$ together provide a succinct representation of $\ADJ[v][u] = \ZERO$ only for contiguous cells $\ADJ[v][u]$ where $u \leq \LAST[v]$ or $v \leq \LAST[u]$: they cannot handle $\ZERO$ anywhere else.
 Unfortunately, in order to extend our construction to support \func{Random-Neighbor} queries using the idea suggested in Algorithm~\ref{alg:naive}, we must unavoidably assign $\ADJ[v][u]$ to $\ZERO$ in random locations beyond $\LAST[v]$ or $\LAST[u]$, which cannot be captured by the current data structure.
 Furthermore, unlike $\ONE$'s, we cannot record $\ZERO$'s using a data structure similarly to that of $P_v$.
 More specifically, to speed-up the sampling process for small $p_{vu}$'s, we must generate many random non-neighbors at once as suggested in Algorithm~\ref{alg:oblivious-coin-toss}, but we cannot afford to spend time linear in the number of created $\ZERO$'s to update our data structure.
 We remedy these issues via the following bucketing approach.
