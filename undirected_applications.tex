\subsection{Applications to Erd\"{o}s-R\'{e}nyi Model and Stochastic Block Model}
\label{sec:applications}
In this section we demonstrate the application of our techniques to
two well known, and widely studied models of randon graphs. That is, as required by Theorem~\ref{thm:grand}, we must provide a method for computing the quantities $\prod_{u=a}^b (1-p_{vu})$ and $\sum_{u=a}^b p_{vu}$ of the desired random graph families in logarithmic time, space and random bits.
Our first implementation focuses on the well known Erd\"{o}s-R\'{e}nyi model -- $G(n,p)$: in this case, $p_{vu} = p$ is uniform and our quantities admit closed-form formulas.
%It is quite simple to calculate the CDF for uniform probabilities.
%{\color{red} Should we still be explicitly mentioning the CDFs? now that we have two ``assumptions'' (product and sum) maybe we should tone this section as actually computing them (rather than just providing CDFs). also did we actually mention sums?}

Next, we focus on the Stochastic Block model with randomly-assigned communities.
\iffalse
{\color{red}In this case, a naive solution would be to simply assign communities to contiguous blocks of indices.
In such a setting, the problem of calculating $\mathsf{F}(v,a,b)$, simply reduces to the $G(n,p)$ case,
with some additional case analysis to check when we are at a community boundary.
However, this setup is unrealistic,
and not particularly useful in the context of the Stochastic Block model.
In fact most algorithms operating on these graphs,
are trying to unveil the underlying community structure.}
{\color{blue} I'd say these motivations should show up in intro or preliminaries (for the full version)}
\fi
Our implementation assigns each vertex to a community in $\{C_1, \ldots, C_r\}$ identically and independently at random, according to some given distribution $\mathsf{R}$ over the communities. We formulate a method of sampling community assignments locally.
This essentially allows us to sample from the \emph{multivariate hypergeometric distribution},
using $\poly(\log n)$ random bits, which may be of independent interest. We remark that, as our first step, we sample for the number of vertices of each community. That is, our construction can alternatively support the community assignment where the number of vertices of each community is given, under the assumption that the \emph{partition} of the vertex set into communities is chosen uniformly at random.

\subsubsection{Erd\"{o}s-R\'{e}nyi Model}
\label{sec:app_er}
As $p_{vu} = p$ for all edges $\{u,v\}$ in the Erd\"{o}s-R\'{e}nyi $G(n,p)$ model, we have the closed-form formulas $\prod_{u=a}^b (1-p_{vu}) = (1-p)^{b-a+1}$ and $\sum_{u=a}^b p_{vu} = (b-a+1)p$, which can be computed in constant time according to our assumption, yielding the following corollary.

\ERGrand*

We remark that there exists an alternative approach that picks $F\sim\mathsf{F}(v,a,b)$ directly via a closed-form formula $a+\lceil\frac{\log U}{\log (1-p)}\rceil$ where $U$ is drawn uniformly from $[0,1)$, rather than binary-searching for $U$ in its CDF. Such an approach may save some $\poly(\log n)$ factors in the resources, given the prefect-precision arithmetic assumption. This usage of the $\log$ function requires $\Omega(n)$-bit precision, which is not applicable to our computation model.

While we are able to generate our random graph on-the-fly supporting all three types of queries, our construction still only requires $\bo(m+n)$ space ($\log n$-bit words) in total at any state; that is, we keep $\bo(n)$ words for $\LAST$, $\bo(1)$ words per neighbor in $P_v$'s, and one marking bit for each block (where there can be up to $m+n$ blocks in total). Hence, our memory usage is nearly optimal for the $G(n,p)$ model:

\EROptimal*

\iffalse
The deterministic version (Section~\ref{sec:ER-det}) does not require the extra overhead resulting from failed iterations.
However, the two level data-structure introduces an extra $\Bo(\log n)$ factor, resulting in the same overall running time.
However, this only requires one $N$-bit random word.
\fi





\subsubsection{Stochastic Block model}
\label{sec:application_sbm}
In the Stochastic Block model, each vertex is assigned to some community $C_i$, $i \in [r]$.
By partitioning the product by communities, we may rewrite the desired formulas, for $v \in C_i$,
as $\prod_{u=a}^b (1-p_{vu}) = \prod_{j=1}^r (1-p_{ij})^{|[a,b]\cap C_j|}$ and $\sum_{u=a}^b p_{vu}=\sum_{j=1}^r |[a,b]\cap C_j|\cdot p_{ij}$.
Thus, it suffices to design a data structure that is able to efficiently count the number of occurrences of vertices of each community
in any contiguous range (namely the value $|[a,b]\cap C_j|$ for each $j \in [r]$),
where the vertices are assigned communities according to a given distribution $\mathsf{R}$.
To this end, we use the following lemma,
yielding an implementation for the Stochastic Block model using $O(r\, \poly(\log n))$ resources per query.

\begin{restatable}{theorem}{res:sbm-data}\label{thm:sbm-data}
There exists a data structure that samples a community for each vertex independently at random from $\mathsf{R}$
with $\frac{1}{\poly(n)}$ error in the $L_1$-distance, and supports queries that ask for the number of occurrences of vertices of each community
in any contiguous range, using $O(r\,\poly(\log n))$ time, random bits, and additional space per query.
Further, this data structure may be implemented in such a way that requires no overhead for initialization.
\end{restatable}
\SBMGrand*

We provide the full details of the construction in Section~\ref{sec:multivariate_hypergeometric_sampling}.
Our construction extends a similar implementation in the work of \cite{huge} which only supports $r = 2$.
The overall data structure is a balanced binary tree, where the root corresponds to the entire range of indices $[n]$,
and the children of each vertex correspond to the first and second half of the parent's range.
Each node\footnote{For clarity, ``vertex'' is only used in the sampled graph, and ``node'' is only used in the internal data structures.}
holds the number of vertices of each community in its range.
The tree initially contains only the root, with the number of vertices of each community $\langle |C_1|, |C_2|,\cdots, |C_r| \rangle$
sampled according to the multinomial distribution (for $n$ samples from the distribution $\mathsf{R}$).
The children are generated top-down on an as-needed basis according to the given queries.
The technical difficulties arise when generating the children,
where one needs to sample the counts assigned to either child from the correct marginal distribution.
We show how to sample such a count from the \emph{multivariate hypergeometric distribution},
below in Theorem~\ref{thm:SamplingManyColors} (proven in Section~\ref{sec:multivariate_hypergeometric_sampling}).

\begin{restatable}{theorem}{SamplingManyColors}
\label{thm:SamplingManyColors}
Given $B$ marbles of $r$ different colors, such that there are $C_i$ marbles of color $i$,
there exists an algorithm that samples $\langle s_1, s_2,\cdots, s_r \rangle$,
the number of marbles of each color appearing when drawing $l$ marbles from the urn without replacement,
in $O(r\cdot\poly(\log B))$ time and random words.
\end{restatable}
\begin{proof}[Proof of Theorem~\ref{thm:sbm-data}]
%We now show that Theorem~\ref{thm:SamplingManyColors} may be used in order to create the following data structure.
Recall that $\mathsf{R}$ denotes the given distribution over integers $[r]$ (namely, the random distribution of communities for each vertex).
Our algorithm generates and maintains random variables $X_1, \ldots, X_n$ (denoting the community assignment),
each of which is drawn independently from $\mathsf{R}$.
Given a pair $(a, b)$, it uses Theorem~\ref{thm:sbm-data} to sample the vector $\vec{C}(a, b) = \langle c_1, \ldots, c_r \rangle$,
where $c_k$ counts the number of variables in $\{X_a, \ldots, X_b\}$ that take on the value $k$.

We maintain a complete binary tree whose leaves corresponds to indices from $[n]$.
Each node represents a range and stores the vector $\vec{C}$ for the corresponding range.
The root represents the entire range $[n]$, which is then halved in each level.
Initially the root samples $\vec{C}(1, n)$ from the multinomial distribution according to $\mathsf{R}$
(see e.g., Section 3.4.1 of \cite{knuth}).
Then, the children are generated on-the-fly as described above.
Thus, each query can be processed within $O(r\,\poly(\log n))$ time, yielding Theorem~\ref{thm:sbm-data}.
\end{proof}

Then, by embedding the information stored by the data structure into the state (as in the proof of Lemma~\ref{lemma:transition}),
we obtain the desired Corollary~\ref{cor:sbm-construct}.
