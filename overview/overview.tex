\section{Overview of our Techniques}
\label{sec:overview_of_our_techniques}
We begin by formalizing our model of \emph{local-access implementations}, inspired by \cite{reut}.

\begin{definition}
\label{def:local_access}
Given a distribution $X$ over a set of huge random objects $\mathbb X$, a \emph{local access implementation}
of a family of query functions $\langle F_1, F_2,\cdots \rangle$ where $F_i: \mathbb X\rightarrow \{0,1\}$,
provides an oracle that returns the value $F_i(X)$ for $X\thicksim \mathbb X$ and a given query $F_i$, while satisfying the following:
%For clarity, we assume that the generator is invoked until its entire graph $G$ is exposed.
%The local-access generator for a probability distribution $\mathsf{D}$ of the desired random graph model must satisfy the following properties:
\begin{itemize}
    \item \textbf{Consistency:}
    All the values $F_i(X)$ returned by the local-access implementation throughout the entire execution
    must be consistent with a single $X\in \mathbb X$.
    \item \textbf{Distribution equivalence:}
    The random object $X\in \mathbb X$ consistent with the responses $\{ F_i(X)\}$ must be sampled from some distribution $\mathsf{X}'$
    that is $\epsilon$-close to the desired distribution $\mathsf{X}$ in $L_1$-distance.
    In this work we focus on supporting $\epsilon = n^{-c}$ for any desired constant $c>0$.
    \item \textbf{Performance:}
    The computation time, random bits, and additional space required to answer a single query must be sub-linear,
    and preferably $\poly(\log n)$ with high probability, without any initialization overhead.
\end{itemize}
\end{definition}

\todo[inline,color=red!80!green!25]{Talk about memory}
In particular, we allow queries to be made adversarially and non-deterministically.
The adversary has full knowledge of the algorithm's behavior and its past random bits.

\todo[inline,color=red!80!green!25]{Talk about LCA type model and public source of randomness.}




\subsection{Basic Tools for Efficient Sampling}
\label{sec:basic_tools_for_efficient_sampling}
In this section, we describe the main techniques used to sample from a distribution $\{ p_d\}$,
which differ based on the type of access to $\{p_d\}$ provided to the algorithm.
If the algorithm is given cumulative distribution function (CDF) queries to $\{p_d\}$,
then it is well known that via $\mathcal O(\log n)$ CDF evaluations, one can sample according
to a distribution that is at most $n^{-c}$ far from $\{p_d\}$ in $L_1$ distance (for constant $c$).

When only given access to queries to the probability dsitribution function (PDF) of $\{p_d\}$, sampling can be more challenging.
The approach that we use in this work is to construct an auxiliary distribution $\{q_d\}$ with the following two properties:
First, $\{ q_d\}$ has an efficiently computable CDF.
Second, $q_d$ approximates $p_d$ pointwise to within a polylogarithmic multiplicative factor for ``most'' of the support of $\{ p_d\}$.
the following Lemma from \cite{huge} formalizes this concept, and shows that if we can provide such a $\{ q_d\}$,
we can quickly sample according to a distribution that is close to $\{ p_d\}$.
\begin{lemma}
\label{lem:rejection_sampling} (From \cite{huge})
Let $\{p_i\}$ and $\{q_i\}$ be distributions satisfying the following conditions:
\begin{enumerate}
    \item There is a poly-time algorithm to approximate $p_i$ and $q_i$ up to $\pm n^{-2}$
    \item Generating an index $i$ according to $q_i$ is closely implementable.
    \item There exists a $poly(log n)$-time recognizable set $B$ such that
    \begin{itemize}
        \item $1-\sum\limits_{i\in B} p_i$ is negligible
        \item There exists a constant $c$ such that for every $i$, it holds that $p_i\le \log^{\mathcal{O}(1)} n\cdot q_i$
    \end{itemize}
\end{enumerate}
Then, generating an index $i$ according to the distribution $\{p_i\}$ is closely-implementable.
\end{lemma}



\input{overview/graphs_overview}
\input{overview/catalan_overview}
\input{overview/coloring_overview}
