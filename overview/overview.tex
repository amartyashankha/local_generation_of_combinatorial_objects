\section{Model and Overview of our Techniques}
\label{sec:overview_of_our_techniques}
We begin by formalizing our model of \emph{local-access implementations}, inspired by \cite{reut}.

\begin{definition}
\label{def:local_access}
Given a distribution $\mathsf  X$ over a set of huge random objects $\mathbb X$, a \emph{local access implementation}
of a family of query functions $\langle F_1, F_2,\cdots \rangle$ where $F_i: \mathbb X\rightarrow \{0,1\}$,
provides an oracle that returns the value $F_i(X)$ for $X\thicksim \mathsf X$ and a given query $F_i$, while satisfying the following:
%For clarity, we assume that the generator is invoked until its entire graph $G$ is exposed.
%The local-access generator for a probability distribution $\mathsf{D}$ of the desired random graph model must satisfy the following properties:
\begin{itemize}
    \item \textbf{Consistency:}
    All the values $F_i(X)$ returned by the local-access implementation throughout the entire execution
    must be consistent with a single $X\in \mathbb X$.
    \item \textbf{Distribution equivalence:}
    The random object $X\in \mathbb X$ consistent with the responses $\{ F_i(X)\}$ must be sampled from some distribution $\mathsf{X}'$
    that is $\epsilon$-close to the desired distribution $\mathsf{X}$ in $L_1$-distance.
    In this work we focus on supporting $\epsilon = n^{-c}$ for any desired constant $c>0$.
    \item \textbf{Performance:}
    The computation time, random bits, and additional space required to answer a single query must be $\poly(\log |X|)$ with high probability,
    without any initialization overhead.
\end{itemize}
\end{definition}

In particular, we allow queries to be made adversarially and non-deterministically.
The adversary has full knowledge of the algorithm's behavior and its past random bits.

\subsection{\textcolor{Maroon}{Memory-less Sampling from Distributions with Huge Description Size}}
\label{sec:memory_less_sampling_from_distributions_with_huge_description_size}
We also consider distributions whose description size is too large to be read by a sublinear algorithm.
In all the problems we have considered so far as well as the ones studied in prior work \cite{huge,sparse,reut}, the description size
of the random object is small (typically $\mathcal O(\log n)$ to represent the size of the instance and a constant number of parameters).
For instance, the $G(n, p)$ model is described using two parameters $n$ and $p$.
However, we wish to implement local access to the uniform distribution over all valid colorings of a given input graph $G$.
Here, the description of the object is the underlying graph $G$, which is too large to be read by a sublinear time algorithm.
Instead, our algorithms access $G$ using local probes.
\begin{definition}
\label{def:local_access_LCA}
Given a combinatorial problem on graphs,
a \emph{local access implementation} of a family of query functions $\langle F_1, F_2,\cdots \rangle$,
provides an oracle $\mathcal A$ with the following properties.
$\mathcal A$ has query access to a graph $G$, and a tape of public random bits $\vec R$.
Assuming that the solution set of the combinatorial problem on $G$ is $\mathbb X$,
$\mathcal A$ upon being queried with $F_i$, returns the value $F_i(X)$ for a specific solution $X\in\mathbb X$ where the choice of $X$
depends only on $\vec R$, and the distribution of $X$ (over $\vec R$) is $\epsilon$-close to the uniform distribution over $\mathbb X$.
Two different instances of $\mathcal A$ with the same graph oracle and the same random bits,
must agree on the choice of $X$ that is consistent with all answered queries regardless of what queries were actually asked.
\end{definition}

This model also has the feature of being memory-less, since independent queries that use the same public randomness
must agree on a single valid coloring, without having to write to memory or communicate in any other way.
We can contrast this definition with the one for \emph{Local Computation Algorithms} \cite{LCA, LCA_space_efficient}
which also allow query access to \emph{some} valid solution and can read the input through local probes.
An additional difficulty in our setting is that we also have to make sure that we return a solution from the correct distribution.
Similarly to LCAs, we can have multiple independent instances of our algorithm answering different queries, but remaining consistent with one another.

\todo[inline,color=red!80!green!25]{Talk about memory}

\todo[inline,color=red!80!green!25]{Talk about LCA type model and public source of randomness.}



\input{overview/graphs_overview}
\input{overview/catalan_overview}
\input{overview/coloring_overview}




\subsection{Basic Tools for Efficient Sampling}
\label{sec:basic_tools_for_efficient_sampling}
In this section, we describe the main techniques used to sample from a distribution $\{ p_d\}$,
which differ based on the type of access to $\{p_d\}$ provided to the algorithm.
If the algorithm is given cumulative distribution function (CDF) queries to $\{p_d\}$,
then it is well known that via $\mathcal O(\log n)$ CDF evaluations, one can sample according
to a distribution that is at most $n^{-c}$ far from $\{p_d\}$ in $L_1$ distance.

When only given access to queries to the probability dsitribution function of $\{p_d\}$, sampling can be more challenging.
The approach that we use in this work is to construct an auxiliary distribution $\{q_d\}$ with the following two properties:
First, $\{ q_d\}$ has an efficiently computable CDF.
Second, $q_d$ approximates $p_d$ pointwise to within a polylogarithmic multiplicative factor for ``most'' of the support of $\{ p_d\}$.
the following Lemma from \cite{huge} formalizes this concept. %, and shows that if we can provide such a $\{ q_d\}$,
%we can quickly sample according to a distribution that is close to $\{ p_d\}$.
\begin{lemma}
\label{lem:rejection_sampling} (From \cite{huge})
Let $\{p_i\}$ and $\{q_i\}$ be distributions on $[n]$ satisfying the following conditions:
\begin{enumerate}
\item There is a poly-time algorithm to approximate $p_i$ and $q_i$ up to $\pm \frac{1}{n^{c+1}}$ for any $i\in [n]$.
    \item We can generate an index $i$ according to a distribution $\{\hat q_d\}$, that is $ \frac{1}{n^c}$ close to $\{q_d\}$ in $L_1$ distance.
    \item There exists a $poly(log n)$-time recognizable set $S$ such that
    \begin{itemize}
        \item $1-\sum\limits_{i\in S} p_i < \frac 1{n^c}$
        \item For every $i\in S$, it holds that $p_i\le \log^{\mathcal{O}(1)} n\cdot q_i$
    \end{itemize}
\end{enumerate}
Then, we can use $\log^{\mathcal O(1)}n$ samples from $\{\hat q_i\}$ to generating an index $i$
according to a distribution that is $ \mathcal O\left(\frac{1}{n^c}\right)$-close to $\{p_i\}$ in $L_1$ distance.
\end{lemma}
\todo[inline,color=red!80!green!25]{Should we prove this?}
