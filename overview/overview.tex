\section{Model and Overview of our Techniques}
\label{sec:overview_of_our_techniques}
We begin by formalizing our model of \emph{local-access implementations}, inspired by \cite{reut},
but we add some aspects that were not addressed in earlier models.
First, we define families of random objects.

\begin{definition}
\label{def:parametrized_random_object}
A \textbf{random object family} maps a description $\Pi$ to a distribution $\mathsf X^{\Pi}$ over the set $\mathbb X^{\Pi}$.
\end{definition}

For example, the \emph{family} of Erd\"{o}s-R\'{e}nyi graphs maps $\Pi = (n, p)$ to a distribution over $\mathbb X^{\Pi}$,
which is the set of all possible $n$-vertex graphs,
where the probability assigned to any graph containing $m$ edges in the distribution $\mathsf X^{\Pi}$ is exactly $p^m\cdot (1-p)^{\binom{n}{2}-m}$.

\begin{definition}
\label{def:local_access}
Given a \textbf{random object family} $\{(\mathsf X^{\Pi}, \mathbb X^{\Pi})\}$ parameterized by $\Pi$, a \emph{local access implementation}
%Given a distribution $\mathsf X^{\Pi}$ over a set of huge random objects $\mathbb X$, a \emph{local access implementation}
of a family of query functions $\langle F_1, F_2,\cdots \rangle$ where $F_i: \mathbb X^{\Pi}\rightarrow \{0,1\}$,
provides an oracle $\mathcal M$ with an internal state for storing a partially generated random object.
Given a description $\Pi$ and a query $F_i$, the oracle returns the value $\mathcal M(\Pi, F_i)$,
and updates its internal state, while satisfying the following:
\begin{itemize}
    \item \textbf{Consistency:}
    There must be a single $X\in \mathbb X^{\Pi}$, such that for all queries $F_i$ presented to the oracle,
    the returned value $\mathcal M(\Pi,F_i)$ equals the true value $F_i(X)$.
    %must be consistent with a single $X\in \mathbb X^{\Pi}$.
    \item \textbf{Distribution equivalence:}
    The random object $X\in \mathbb X^{\Pi}$ consistent with the responses $\{ F_i(X)\}$ must be sampled from some distribution $\hat{\mathsf{X}}^{\Pi}$
    that is $\epsilon$-close to the desired distribution $\mathsf{X}^{\Pi}$ in $L_1$-distance.
    In this work, we focus on supporting $\epsilon = n^{-c}$ for any desired constant $c>0$.
    \item \textbf{Performance:}
    The computation time, and random bits required to answer a single query must be sub-linear in $|X|$ with high probability,
    without any initialization overhead.
\end{itemize}
In particular, we allow queries to be made adversarially and non-deterministically.
The adversary has full knowledge of the algorithm's behavior and its past random bits.
\end{definition}

For example, in the $G(n,p)$ family with description $\Pi = (n, p)$,
we can define \func{Vertex-Pair} query functions $\{ F_{(u,v)}\}_{u,v\in [n]}$.
So, given a graph $G\in \mathbb X^{\Pi}$, the query $F_{(u,v)}(G) = 1$ if and only if $(u,v)\in G$.

In prior work \cite{reut, huge, sparse} as well as some of our results, the input description $\Pi$ is of small size (typically $\mathcal O(\log n)$),
and the oracle $\mathcal M$ can read all of $\Pi$ (for example, $\Pi = (n, p)$ in $G(n,p)$).

\subparagraph*{{\Large Distributions with Huge Description Size:}}
\label{par:distributions_with_huge_description_size}
We initiate the study of \textbf{random object families} where the description $\Pi$ is too large to be read by a sub-linear time algorithm.
%In all the problems considered in prior work \cite{huge,sparse,reut}, the description size
%of the random object is small, typically $\mathcal O(\log n)$ to represent the size of the instance and a constant number of parameters.
%For instance, the $G(n, p)$ model is described using two parameters $n$ and $p$.
%However, if one wishes to implement local access to the uniform distribution over all valid colorings of a given input graph $G$,
In this setting, the oracle from Definition~\ref{def:local_access} cannot read the entire input $\Pi$, and instead accesses it through local probes.
For instance, consider the \textbf{random object family} that maps a graph $G$ to the uniform distribution over valid colorings of $G$.
Here, the description $\Pi$ includes the entire graph $G$, which is too large to be read by a sub-linear time algorithm.
In this case, the oracle can query the underlying graph using neighborhood probes.
The number of such probes used to answer a single query must be sub-linear in the input size.


\paragraph*{Supporting Independent Query Oracles: Memory-less Implementations}
\label{par:supporting_independent_query_oracles_memory_less_implementations}
The model in Definition~\ref{def:local_access} only supports sequential queries,
since the response to a future query may depend on the changes in internal state caused by past queries.
In some applications, we may want to have multiple independent query oracles whose responses are all consistent with each other.
One way to achieve this is to restrict our attention to \emph{memory-less} implementations; ones without any internal state.
An important implication of being memory-less is that the responses to each query is oblivious to the order of queries being asked.
In fact, the lack of internal state implies that independent implementations that use the same random bits and the same input description
must respond to queries in the same way.
Thus, instead of using the internal state to maintain consistency, memory-less implementations are given access to the same public random oracle.

%In addition to the above generalization of Definition~\ref{def:local_access}, we also consider ``memory-less'' oracle implementations,
%where the oracle does not have any internal state, and instead has access to a source of common randomness.
%An important implication of being memory-less is that the responses to each query is oblivious of the order of queries being asked.
%In fact, the lack of internal state implies that independent implementations that use the same common randomness and the same input description
%must respond to queries in the same way.
%So, we can have multiple (and even simultaneous) copies of the implementation,
%that are all consistent with a single random object drawn from the distribution,
%without having to write to shared memory or communicate in any other way.

For the problem of sampling a random graph coloring,
we present an implementation that is memory-less and also accesses the input description through local probes,
as elaborated in the following model:

\begin{definition}
\label{def:local_access_LCA}
Given a \textbf{random object family} $\{(\mathsf X^{\Pi}, \mathbb X^{\Pi})\}$ parameterized by input $\Pi$,
a \emph{local access implementation} of a family of query functions $\langle F_1, F_2,\cdots \rangle$,
provides an oracle $\mathcal M$ with the following properties.
$\mathcal M$ has query access to the input description $\Pi$, and a tape of public random bits $\vec R$.
Upon being queried with $\Pi$ and $F_i$, the oracle uses sub-linear resources to return the value $\mathcal M(\Pi,\vec R,F_i)$,
which must equal $F_i(X)$ for a specific $X\in\mathbb X^{\Pi}$, where the choice of $X$ depends only on $\vec R$,
and the distribution of $X$ (over $\vec R$) is $\frac1{n^c}$-close to the distribution over $\mathbb X^{\Pi}$, for any given constant $c$.
Thus, different instances of $\mathcal M$ with the same description $\Pi$ and the same random bits $\vec R$,
must agree on the choice of $X$ that is consistent with all answered queries regardless of the order and content of queries that were actually asked.
\end{definition}

We can contrast Definition~\ref{def:local_access_LCA} with the one for \emph{Local Computation Algorithms} \cite{LCA, LCA_space_efficient}
which also allow query access to \emph{some} valid solution by reading the input through local probes.
The additional challenge in our setting is that we also have to make sure that we return a uniformly random solution, rather than an arbitrary one.
%Similarly to LCAs, we can have multiple independent instances of our algorithm answering different queries, but remaining consistent with one another.
We also note that the memory-less property may be achieved for small description size \textbf{random object families}.
For instance, our implementation for the directed small world model admits such a memory-less implementation using public random bits.



\input{overview/graphs_overview}
\input{overview/catalan_overview}
\input{overview/coloring_overview}




\subsection{Basic Tools for Efficient Sampling}
\label{sec:basic_tools_for_efficient_sampling}
In this section, we describe the main techniques used to sample from a distribution $\{ p_i\}_{i\in [n]}$,
which differs based on the type of access to $\{p_i\}$ provided to the algorithm.
If the algorithm is given cumulative distribution function (CDF) access to $\{p_i\}$,
then it is well known that via $\mathcal O(\log n)$ CDF evaluations, one can sample according
to a distribution that is at most $n^{-c}$ far from $\{p_i\}$ in $L_1$ distance.

Sampling can be more challenging when when we can only access the probability distribution function (PDF) of $\{p_i\}$,
The approach that we use in this work is to construct an auxiliary distribution $\{q_i\}$ such that:
(1) $\{ q_i\}$ has an efficiently computable CDF, and
(2) $q_i$ approximates $p_i$ pointwise to within a polylogarithmic multiplicative factor for ``most'' of the support of $\{ p_i\}$.
The following Lemma inspired by \cite{huge} formalizes this concept.
\begin{lemma}
\label{lem:rejection_sampling}
Let $\{p_i\}_{i\in [n]}$ and $\{q_i\}_{i\in [n]}$ be distributions on $[n]$ satisfying the following conditions:
\begin{enumerate}
    \item There is a $\log^{\mathcal O(1)}n$ time algorithm to approximate $p_i$ and $q_i$
    up to a multiplicative $\left(1\pm \frac{1}{n^c}\right)$ factor.
    \item We can generate an index $i$ according to a distribution $\{\hat q_i\}$,
    where $\hat q_i$ is a $\left(1\pm \frac{1}{n^c}\right)$ multiplicative approximation to $q_i$.
    \item There exists a $poly(\log n)$-time recognizable set $S$ such that
    \begin{itemize}
        \item $1-\sum\limits_{i\in S} p_i < \frac 1{n^c}$
        \item For every $i\in S$, it holds that $p_i\le \log^{\mathcal{O}(1)} n\cdot q_i$
    \end{itemize}
\end{enumerate}
Then, with high probability we can use only $\log^{\mathcal O(1)}n$ samples from $\{\hat q_i\}$ to generate an index $i$
according to a distribution that is $ \mathcal O\left(\frac{1}{n^c}\right)$-close to $\{p_i\}$ in $L_1$ distance.
\end{lemma}
\begin{proof}
We begin by setting an upper bound $U = \log^{\mathcal O(1)}n$ on $p_i/q_i$ for all $i\in S$.
The sampling proceeds in iterations, such that in each iteration we obtain an index $i$ according to the distribution $\{ \hat q_i\}$.
If $i\not\in S$, this index is returned with probability $\tilde p_i/U\tilde q_i$,
where $\tilde p_i$ and $\tilde q_i$ are the $\left( 1\pm \frac{1}{n^c} \right)$ multiplicative approximations to $p_i$ and $q_i$,
Otherwise, we repeat this process until some output is returned.

The probability of returning index $i\in S$ in a particular iteration is $\hat q_i\cdot (\tilde p_i/U \tilde q_i)$,
which is in turn a $\left( 1\pm \frac{1}{n^c} \right)$ multiplicative approximation to $p_i/U$.
Hence, the probability of success in a single iteration is roughly $1/U$,
and therefore we only need $\mathcal O(U\log n) = \log^{\mathcal O(1)}n$ iterations (and the same number of samples from $\{ \hat q_i\}$)
in order to succeed with high probability.
The resulting distribution of indices approximates $\{ p_i\}$ pointwise on the domain $S$, up to a factor of $\left( 1\pm \frac{1}{n^c} \right)$.
Since the remainder of the domain contains at most $ \frac{1}{n^c}$ probability mass,
the output distribution is $\mathcal O\left(\frac{1}{n^c}\right)$ close to $\{ p_i\}$ in $L_1$ distance.
\end{proof}
