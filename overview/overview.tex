\section{Model and Overview of our Techniques}
\label{sec:overview_of_our_techniques}
We begin by formalizing our model of \emph{local-access implementations}, inspired by \cite{reut}.

\begin{definition}
\label{def:local_access}
Given a distribution $\mathsf  X$ over a set of huge random objects $\mathbb X$, a \emph{local access implementation}
of a family of query functions $\langle F_1, F_2,\cdots \rangle$ where $F_i: \mathbb X\rightarrow \{0,1\}$,
provides an oracle that returns the value $F_i(X)$ for $X\thicksim \mathsf X$ and a given query $F_i$, while satisfying the following:
%For clarity, we assume that the generator is invoked until its entire graph $G$ is exposed.
%The local-access generator for a probability distribution $\mathsf{D}$ of the desired random graph model must satisfy the following properties:
\begin{itemize}
    \item \textbf{Consistency:}
    All the values $F_i(X)$ returned by the local-access implementation throughout the entire execution
    must be consistent with a single $X\in \mathbb X$.
    \item \textbf{Distribution equivalence:}
    The random object $X\in \mathbb X$ consistent with the responses $\{ F_i(X)\}$ must be sampled from some distribution $\mathsf{X}'$
    that is $\epsilon$-close to the desired distribution $\mathsf{X}$ in $L_1$-distance.
    In this work we focus on supporting $\epsilon = n^{-c}$ for any desired constant $c>0$.
    \item \textbf{Performance:}
    The computation time, random bits, and additional space required to answer a single query must be $\poly(\log |X|)$ with high probability,
    without any initialization overhead.
\end{itemize}
\end{definition}

In particular, we allow queries to be made adversarially and non-deterministically.
The adversary has full knowledge of the algorithm's behavior and its past random bits.

\paragraph*{Memory-less Sampling from Distributions with Huge Description Size}
\label{par:memory_less_sampling_from_distributions_with_huge_description_size}
We also consider distributions whose description size is too large to be read by a sublinear algorithm.
In all the problems we have considered so far as well as the ones studied in prior work \cite{huge,sparse,reut}, the description size
of the random object is small (typically $\mathcal O(\log n)$ to represent the size of the instance and a constant number of parameters).
For instance, the $G(n, p)$ model is described using two parameters $n$ and $p$.
However, we wish to implement local access to the uniform distribution over all valid colorings of a given input graph $G$.
Here, the description of the object is the underlying graph $G$, which is too large to be read by a sublinear time algorithm.
Instead, our algorithms access $G$ using local probes.
\begin{definition}
\label{def:local_access_LCA}
Given a combinatorial problem on graphs,
a \emph{local access implementation} of a family of query functions $\langle F_1, F_2,\cdots \rangle$,
provides an oracle $\mathcal A$ with the following properties.
$\mathcal A$ has query access to a graph $G$, and a tape of public random bits $\vec R$.
Assuming that the solution set of the combinatorial problem on $G$ is $\mathbb X$,
$\mathcal A$ upon being queried with $F_i$, returns the value $F_i(X)$ for a specific solution $X\in\mathbb X$ where the choice of $X$
depends only on $\vec R$, and the distribution of $X$ (over $\vec R$) is $\epsilon$-close to the uniform distribution over $\mathbb X$.
Two different instances of $\mathcal A$ with the same graph oracle and the same random bits,
must agree on the choice of $X$ that is consistent with all answered queries regardless of what queries were actually asked.
\end{definition}

This model also has the feature of being memory-less, since independent queries that use the same public randomness
must agree on a single valid coloring, without having to write to memory or communicate in any other way.
We can contrast this definition with the one for \emph{Local Computation Algorithms} \cite{LCA, LCA_space_efficient}
which also allow query access to \emph{some} valid solution and can read the input through local probes.
An additional difficulty in our setting is that we also have to make sure that we a uniformly random solution (as opposed to an arbitrary one).
Similarly to LCAs, we can have multiple independent instances of our algorithm answering different queries, but remaining consistent with one another.



\input{overview/graphs_overview}
\input{overview/catalan_overview}
\input{overview/coloring_overview}




\subsection{Basic Tools for Efficient Sampling}
\label{sec:basic_tools_for_efficient_sampling}
In this section, we describe the main techniques used to sample from a distribution $\{ p_d\}$,
which differ based on the type of access to $\{p_d\}$ provided to the algorithm.
If the algorithm is given cumulative distribution function (CDF) queries to $\{p_d\}$,
then it is well known that via $\mathcal O(\log n)$ CDF evaluations, one can sample according
to a distribution that is at most $n^{-c}$ far from $\{p_d\}$ in $L_1$ distance.

When only given access to queries to the probability distribution function of $\{p_d\}$, sampling can be more challenging.
The approach that we use in this work is to construct an auxiliary distribution $\{q_d\}$ with the following two properties:
First, $\{ q_d\}$ has an efficiently computable CDF.
Second, $q_d$ approximates $p_d$ pointwise to within a polylogarithmic multiplicative factor for ``most'' of the support of $\{ p_d\}$.
the following Lemma from \cite{huge} formalizes this concept. %, and shows that if we can provide such a $\{ q_d\}$,
%we can quickly sample according to a distribution that is close to $\{ p_d\}$.
\begin{lemma}
\label{lem:rejection_sampling} (From \cite{huge})
Let $\{p_i\}$ and $\{q_i\}$ be distributions on $[n]$ satisfying the following conditions:
\begin{enumerate}
\item There is a poly-time algorithm to approximate $p_i$ and $q_i$ up to $\pm \frac{1}{n^{c+1}}$ for any $i\in [n]$.
    \item We can generate an index $i$ according to a distribution $\{\hat q_d\}$, that is $ \frac{1}{n^c}$ close to $\{q_d\}$ in $L_1$ distance.
    \item There exists a $poly(log n)$-time recognizable set $S$ such that
    \begin{itemize}
        \item $1-\sum\limits_{i\in S} p_i < \frac 1{n^c}$
        \item For every $i\in S$, it holds that $p_i\le \log^{\mathcal{O}(1)} n\cdot q_i$
    \end{itemize}
\end{enumerate}
Then, we can use $\log^{\mathcal O(1)}n$ samples from $\{\hat q_i\}$ to generating an index $i$
according to a distribution that is $ \mathcal O\left(\frac{1}{n^c}\right)$-close to $\{p_i\}$ in $L_1$ distance.
\end{lemma}
\todo[inline,color=red!80!green!25]{Should we prove this?}
