\subsection{Undirected Graphs}
\label{sec:undirected_graphs}
We consider the generic class of \emph{undirected graphs} with {\em independent edge probabilities} $\left\{ p_{uv} \right\}_{u,v\in V}$,
(where $p_{uv}$ denotes the probability that there is an edge between $u$ and $v$),
from which the results can be applied to Erdos-Renyi random graphs and the Stochastic Block Model.
Throughout, we identify our vertices via their unique IDs from $1$ to $n$, namely $V = [n]$.
In this model, \func{Vertex-Pair} queries by themselves can be implemented trivially,
since the existence of any edge $(u,v)$ is an independent Bernoulli random variable,
but it becomes harder to maintain consistency when implementing them in conjunction with the other queries.
Inspired by \cite{reut}, we provide an implementation of $\func{next-neighbor}$ queries,
which return the neighbors of any given vertex one by one in lexicographic order.
Finally, we introduce a new query: \func{Random-Neighbor} that returns a uniformly random neighbor of any given vertex.
This would be useful for any algorithm that performs random walks.
$\func{Random-Neighbor}$ queries in non-sparse graphs present particularly interesting challenges that are outlined below.

\paragraph*{\func{Next-Neighbor} Queries}
\label{par:next_neighbor_queries}
In Erdos-Renyi graphs, the (lexicographically) next neighbor of a vertex can be recovered by generating consecutive entries
of the adjacency matrix until a neighbor is found, which takes roughly $\Omega(1/p_{uv})$ time.
For small edge probabilities $p_{uv} = o(1)$, this implementation is inefficient, and we show how to improve the runtime to $\mathcal O(\poly(\log n))$.
Our main technique is to sample the number of ``non-neighbors'' preceding the next neighbor.
To do this, we assume that we can estimate the ``skip'' probabilities $F(v,a,b)=\prod^{b}_{u=a} (1-p_{vu})$,
where $F(v,a,b)$ is the probability that $v$ has no neighbors in the range $[a,b]$.
We later show how to compute this quantity efficiently for $G(n,p)$ and the Stochastic Block Model.

This strategy of \emph{skip-sampling} is also used in \cite{reut}.
However, in our work, the main difficulty arises from the fact that our graph is undirected,
and thus we must ``inform'' all (potentially $\Theta(n)$) non-neighbors once we decide on the query vertex's next neighbor.
Concretely, if $u'$ is sampled as the next neighbor of $v$ after its previous neighbor $u$,
we must maintain consistency in subsequent steps by ensuring that none of the vertices in the range $(u,u')$ return $v$ as a neighbor.
This update will become even more complicated as we handle \func{Random-Neighbor} queries, where we may generate non-neighbors at random locations.

In Section~\ref{sec:ER-rand}, we present a randomized implementation (Algorithm~\ref{alg:oblivious-coin-toss})
that supports \func{Next-Neighbor} queries efficiently, albeit the analysis of its performance is rather complicated.
We remark that this approach may be extended to support \func{Vertex-Pair} queries with superior performance
(given that we do not to support \func{Random-Neighbor} queries) and to provide deterministic resource usage guarantee
-- the full analysis can be found in Section~\ref{sec:reroll-cont} and \ref{sec:ER-det}, respectively.


\paragraph*{\func{Random-Neighbor} Queries}
\label{par:random_neighbor_queries}

We provide \func{Random-Neighbor} queries (Section~\ref{sec:blocks}) using $\poly(\log n)$ resources.
The ability to do so is surprising since:
(1) Sampling the degree of the query vertex, we suspect, is not viable for \emph{sub-linear} implementations, because this quantity alone
imposes dependence on the existence of \emph{all} of its potential incident edges and on the rest of the graph.
\todo[inline]{Why?}
Therefore, our implementation needs to return a random neighbor, with probability reciprocal to the query vertex's degree,
without resorting to ``knowing'' its degree.
(2) Even without committing to the degrees, answers to \func{Random-Neighbor} queries
affect the conditional probabilities of the remaining adjacencies in a global and non-trivial manner.
\footnote{\label{conditional}Consider a $G(n,p)$ graph with small $p$, say $p = 1/\sqrt n$,
such that vertices will have $\widetilde{\mathcal{O}}(\sqrt n)$ neighbors with high probability.
After $\widetilde{\mathcal{O}}(\sqrt n)$ \func{Random-Neighbor} queries, we will have uncovered all the neighbors (w.h.p.),
so that the conditional probability of the remaining $\Theta(n)$ edges should now be close to zero.}

We formulate an approach which samples multiple consecutive edges at once,
in such a way that the conditional probabilities of the unsampled edges remain independent and ``well-behaved'' during subsequent queries.
For each vertex $v$, we divide the potential neighbors of $v$ into consecutive ranges $\{B^{(i)}_v\}$ called blocks,
so that each $B^{(i)}_v$ contains $\Theta(1)$ neighbors in expectation (i.e. $\sum_{u\in B_i} p_{vu} = \Theta(1)$).
The subroutine of \func{Next-Neighbor} is applied to sample the neighbors within a block in expected $\mathcal{\widetilde O}(1)$ time.
We can now obtain a neighbor of $v$ by picking a random neighbor from a random block,
but this introduces a bias because all blocks may not have the same number of neighbors.
We remove this bias by rejecting samples from block $B^{(i)}_v$ with probability proportional to the number of neighbors in $B^{(i)}_v$.




\subsubsection{Applications to Random Graph Models}
\label{sec:applications_to_random_graph_models}
We now consider the application of our construction above to actual random graph models,
where we must realize the assumption that $\prod^{b}_{u=a} (1-p_{vu})$ and $\sum^{b}_{u=a} p_{vu}$ can be computed efficiently.
For the Erd\"{o}s-R\'{e}nyi $G(n,p)$ model, these quantities have simple closed-form expressions.
Thus, we obtain implementations of $\func{Vertex-Pair}$, $\func{Next-Neighbor}$, and $\func{Random-Neighbor}$ queries,
using polylogarithmic resources (time, space and random bits) per query, for \emph{arbitrary} values of $p$,
We remark that, while $\Omega(n+m) = \Omega(p n^2)$ time and space is clearly necessary to generate and represent a full random graph,
our implementation supports local-access via all three types of queries, and yet can generate a full graph in $\widetilde{O}(n+m)$ time and space
(Corollary~\ref{thm:er-optimal}), which is tight up to polylogarithmic factors.

We also generalize our construction to implement the Stochastic Block Model.
In this model, the vertex set is partitioned into $r$ \emph{communities} $\left\{ C_1, \ldots, C_r \right\}$.
The probability that an edge exists between $u\in C_i$ and $v \in C_j$ is $p_{ij}$.
As communities in the observed data are generally unknown a priori,
and significant research has been devoted to designing efficient algorithms for community detection and recovery, these studies generally consider
the \emph{random community assignment} condition for the purpose of designing and analyzing algorithms (see e.g., \cite{mossel2015reconstruction}).
Thus, we aim to construct implementations where the community assignment of vertices are independently sampled from some given distribution
$\mathsf{R}$\footnote{Our algorithm also supports the alternative specification where the community sizes $\langle |C_1|, \ldots, |C_r|\rangle$
are given instead, where the assignment of vertices $V$ into these communities is chosen uniformly at random.}.
The difficulty here is to obtain a uniformly sampled assignment of vertices to communities on-the-fly.

Our approach is, as before, to sample for the next neighbor or a random neighbor directly,
although our result does not simply follow closed-form formulas,
as the probabilities for the potential edges now depend on the communities of endpoints.
To handle this issue, we observe that it is sufficient to efficiently count
the number of vertices of each community in any range of contiguous vertex indices.
We then design a data structure extending a construction of \cite{huge}, which maintain these counts for ranges of vertices,
and ``sample'' the partition of their counts only on an as-needed basis.
This extension results in an efficient technique to sample counts from the \emph{multivariate hypergeometric distribution}
(Section~\ref{sec:multivariate_hypergeometric_sampling}) which may be of independent interest.
For $r$ communities, this yields an implementation with $ \mathcal{O}(r\cdot \poly(\log n))$ overhead in required resources for each operation.




\subsection{Directed Graphs}
\label{sec:directed_graphs}
Lastly, we consider Kleinberg's Small World model (\cite{kleinberg, klein}) in Section~\ref{sec:small_world}.
While Small-World models are proposed to capture properties of observed data such as small shortest-path 
distances and large clustering coefficients \cite{watts1998collective}, 
this important special case of Kleinberg's model, defined on two-dimensional grids, demonstrates underlying geographical structures of networks.

In this model, each vertex is identified via its 2D coordinate $v = (v_x, v_y) \in [\sqrt{n}]^2$.
Define the Manhattan distance as $\func{dist}(u,v)=|u_x-v_x|+|u_y-v_y|$,
and the probability that each directed edge $(u,v)$ exists is $c/(\func{dist}(u,v))^{2}$.
A common choice for $c$ is given by normalizing the distribution such that the expected out-degree of each vertex is 1 ($c = \Theta(1/\log n)$).
We can also support a range of values of $c=\log^{\pm\Theta(1)}n$.
Since the degree of each vertex in this model is $\Bo(\log n)$ with high probability, we design implementations supporting \func{All-Neighbor} queries.
In contrast to our previous cases, this model imposes an underlying two-dimensional structure of the vertex set,
which governs the distance function as well as complicates the individual edge probabilities.

We design generators for the aforementioned case of the Small-World model, supporting each \func{all-neighbors} query,
listing all neighbors from closest to furthest away from the queried vertex, using $\poly(\log n)$ resources per query.
Providing local access for directed graphs is simpler because the out-neighbors of vertices may be chosen independently at each vertex.
So, the main challenge is to sample for the next (closest) neighbor, when the probabilities are a function of the Manhattan distance on the lattice.
Rather than sampling for a neighbor directly, we sample the next smallest distance with a neighbor,
employing the rejection sampling technique that allows efficient sampling through an approximate distribution that have closed-form description,
then as a second step, sample for all neighbors for each chosen distance.
